{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "DvH9aWTgf4Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Import packages"
      ],
      "metadata": {
        "id": "WwEOY8c6bAan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "id": "0-VtbO6HKdwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9607d58-429a-46f8-8845-be94b2c12998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary packages\n",
        "import os\n",
        "import random\n",
        "# import copy\n",
        "from statistics import mean\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "QxjsI48Pq3bH"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "from torch.profiler import profile, record_function, schedule, ProfilerActivity\n",
        "\n",
        "import torchvision\n",
        "# from torchvision.io import read_image\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.ops import box_convert\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms.functional import to_pil_image"
      ],
      "metadata": {
        "id": "encOC-cAsdqm"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Augmentation\n",
        "import albumentations as A\n",
        "# from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "metadata": {
        "id": "-tcOEOWVAohJ"
      },
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(A.__version__)"
      ],
      "metadata": {
        "id": "q27WEDjeshXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b43c7b-3467-427f-8c3f-c9d77b60c63c"
      },
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "0.13.1+cu113\n",
            "1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set reproducibility\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "# torch.cuda.manual_seed(SEED)\n",
        "# torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "w8HFhEJCzJaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf41696f-6b7a-4ebb-ac2a-56bf296f3cd4"
      },
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc9131d02f0>"
            ]
          },
          "metadata": {},
          "execution_count": 432
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH = '/content/drive/MyDrive/ML_Projects/How_many_Sparrows'\n",
        "DATA_PATH = os.path.join(PROJECT_PATH, 'data')\n",
        "IMAGE_PATH, BBOX_DATA_PATH = [os.path.join(DATA_PATH, 'raw', data_path) for data_path in ['images', 'bboxes/bounding_boxes.csv']]\n",
        "TRAIN_FILE_PATH, TEST_FILE_PATH = [os.path.join(DATA_PATH, 'prepared', csv_file) for csv_file in ['train.csv', 'test.csv']]"
      ],
      "metadata": {
        "id": "QWqiVvCPQx60"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to be used to run the model\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Set profile parameters\n",
        "PROFILE_SCHEDULE = schedule(skip_first=1, wait=2, warmup=1, active=1)\n",
        "PROFACTIVITY = ProfilerActivity.CUDA if DEVICE == 'cuda' else ProfilerActivity.CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMetjlQcCOiL",
        "outputId": "11b6a91b-18bc-456e-96df-609ff3e2ca13"
      },
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/profiler/profiler.py:231: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
            "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Helper functions"
      ],
      "metadata": {
        "id": "cOx1zWcxau83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_transforms(aug_file_name=''):\n",
        "    \"\"\"Returns a Albumentation object and saves it to a JSON file \n",
        "    if aug_file_name is specified.\"\"\"\n",
        "    aug = A.ReplayCompose([\n",
        "                           A.LongestMaxSize(1333, always_apply=True),  \n",
        "                           A.SmallestMaxSize(800, always_apply=True),\n",
        "                           A.RandomSizedBBoxSafeCrop(800, 800, p=0.3),\n",
        "                           A.Rotate(180, p=0.4, border_mode=cv2.BORDER_CONSTANT, crop_border=True),\n",
        "                           A.HorizontalFlip(p=0.6),\n",
        "                           A.VerticalFlip(p=0.4),\n",
        "                           A.ColorJitter(0.5, 0.5, 0.5, 0, p=0.7),\n",
        "                           A.RandomRain(p=0.5),\n",
        "                           A.OneOrOther(\n",
        "                               A.Blur(10, p=0.7),\n",
        "                               A.GaussianBlur((11, 21), p=0.3),\n",
        "                               p=0.6\n",
        "                               ),\n",
        "                           ], \n",
        "                          A.BboxParams(format='coco', min_visibility=0.1, label_fields=['labels']),\n",
        "                          p=0.8)\n",
        "    \n",
        "    if aug_file_name:\n",
        "        A.save(aug, os.path.join(PROJECT_PATH, 'results', aug_file_name)) \n",
        "\n",
        "    return aug"
      ],
      "metadata": {
        "id": "1iAxII2iHotX"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_group_train_test_split(data, stratification_basis, groups):\n",
        "    \"\"\"Stratified splits data into training and test sets,\n",
        "    taking into account groups, and returns the corresponding indices.\"\"\"\n",
        "    split = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=0)\n",
        "    train_ids, test_ids = next(split.split(X=data, y=stratification_basis, groups=groups))\n",
        "    return train_ids, test_ids"
      ],
      "metadata": {
        "id": "FGjaEuUyakHA"
      },
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Object Detection Models"
      ],
      "metadata": {
        "id": "mvbzVAT4DZDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Pytorch"
      ],
      "metadata": {
        "id": "pdsuenUxDpr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1. Helper Functions"
      ],
      "metadata": {
        "id": "tsemXBCYtYB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    \"\"\"Collate batches in Dataloader.\"\"\"\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "if3MglFsMKlR"
      },
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_trace(prof):\n",
        "    prof_out = prof.key_averages().table(sort_by=f'{DEVICE}_memory_usage', row_limit=10)\n",
        "    print(prof_out)\n",
        "    trace_path = os.path.join(PROJECT_PATH, 'tmp')\n",
        "\n",
        "    if not os.path.exists(trace_path):\n",
        "        os.mkdir(trace_path)\n",
        "\n",
        "    prof.export_chrome_trace(os.path.join(trace_path, f'model_inference_trace_{prof.step_num}.json')) # chrome://tracing/"
      ],
      "metadata": {
        "id": "LYixaZhp_pWt"
      },
      "execution_count": 438,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def draw_bboxes_on_image(img, bboxes, scores=None):\n",
        "    \"\"\"Draws an image with bounding boxes from Tensors.\"\"\"\n",
        "    img_box = draw_bounding_boxes(img, boxes=bboxes, colors='orange', width=2)\n",
        "    img = to_pil_image(img_box.detach())\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    ax = plt.gca()\n",
        "    if scores is not None:\n",
        "        for bb, sc in zip(bboxes, scores):\n",
        "            x, y = bb.tolist()[:2]\n",
        "            text_sc = f\"{sc:0.2f}\"\n",
        "            ax.text(x, y, text_sc , fontsize=12, \n",
        "                    bbox=dict(facecolor='orange', alpha=0.5))\n",
        "            \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "e99Q7prhLHJG"
      },
      "execution_count": 439,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(dataloader, model, optimizer, device='cpu'):\n",
        "    \"\"\"Passes a training step in one epoch.\"\"\"\n",
        "    accum_dict_losses = {}\n",
        "    accum_model_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "    dataloader_progress = tqdm(dataloader, colour='blue')\n",
        "\n",
        "    # Set a model to the training mode\n",
        "    model.train()\n",
        "\n",
        "    for images, targets in dataloader_progress:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Сompute a model batch losses\n",
        "        batch_dict_losses = model(images, targets)\n",
        "        batch_model_loss = sum([loss for loss in batch_dict_losses.values()])\n",
        "\n",
        "        # Accumulate statistics for computing the average losses per epoch\n",
        "        accum_dict_losses.update({\n",
        "            k: accum_dict_losses.get(k, 0) + v.item() for k, v in batch_dict_losses.items()\n",
        "            })\n",
        "        accum_model_loss += batch_model_loss.item()\n",
        "\n",
        "        # Optimize the model parameters\n",
        "        optimizer.zero_grad()\n",
        "        batch_model_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Compute the average losses\n",
        "    epoch_dict_losses = {k: v / num_batches for k, v in accum_dict_losses.items()}\n",
        "    epoch_model_loss = accum_model_loss / num_batches \n",
        "  \n",
        "    return {'epoch_dict_losses': epoch_dict_losses, \n",
        "            'epoch_loss': epoch_model_loss}"
      ],
      "metadata": {
        "id": "zuREtVaBucDq"
      },
      "execution_count": 440,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def eval_one_epoch(dataloader, model, device='cpu'):\n",
        "    \"\"\"Passes a inference evaluation step in one epoch.\"\"\"\n",
        "    accum_ratio_detected_obj = 0\n",
        "    accum_model_score = 0\n",
        "    results = []\n",
        "    num_batches = len(dataloader)    \n",
        "    dataloader_progress = tqdm(dataloader, colour='blue')\n",
        "    \n",
        "    # Set a model to the evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for images, targets in dataloader_progress:\n",
        "        images = [img.to(device) for img in images]\n",
        "\n",
        "        # Get prediction results\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Сompute a model batch statistics\n",
        "        batch_num_gt_boxes = [len(t['boxes']) for t in targets]\n",
        "        batch_num_pred_boxes = [len(out['boxes'][out['labels'] == 1]) for out in outputs]\n",
        "        batch_model_score = 0\n",
        "\n",
        "        if sum(batch_num_pred_boxes) != 0:\n",
        "            batch_model_score = mean(\n",
        "                [mean(out['scores'][out['labels'] == 1].tolist()[:gt+1]) \n",
        "                    for out, gt in zip(outputs, batch_num_gt_boxes)])\n",
        "        \n",
        "        # Accumulate statistics for computing the average values per epoch        \n",
        "        accum_ratio_detected_obj += sum(batch_num_pred_boxes) / sum(batch_num_gt_boxes)\n",
        "        accum_model_score += batch_model_score\n",
        "\n",
        "        # Convert the images to RGB format and add to the outputs\n",
        "        for img, out in zip(images, outputs):\n",
        "            out.update(image_rgb=img.mul(255).to(torch.uint8))\n",
        "\n",
        "        results += outputs\n",
        "    \n",
        "    # Compute the average score and the ratio of detected objects \n",
        "    epoch_ratio_detected_obj = accum_ratio_detected_obj / num_batches\n",
        "    epoch_model_score = accum_model_score / num_batches\n",
        "\n",
        "    return {'epoch_precent_detected_obj': epoch_ratio_detected_obj,\n",
        "            'epoch_score': epoch_model_score, \n",
        "            'results': results}"
      ],
      "metadata": {
        "id": "E_0nny0tKfMF"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_image_with_model_outputs(model_results, draw_score_threshold=0, image_index=None):\n",
        "    \"\"\"Draws a random or given image with output boxes and scores greater than draw_score_threshold.\"\"\"\n",
        "    if image_index is None:\n",
        "        image_index = random.randint(0, len(model_results['results'])-1)\n",
        "\n",
        "    sample_model_res = model_results['results'][image_index]\n",
        "    sample_draw = sample_model_res['scores'] > draw_score_threshold\n",
        "    draw_bboxes_on_image(sample_model_res['image_rgb'], sample_model_res['boxes'][sample_draw], \n",
        "                         sample_model_res['scores'][sample_draw])   "
      ],
      "metadata": {
        "id": "dG_tjFsc6DOh"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_state(model_to_save, save_model_dir, best_stat_name):\n",
        "    \"\"\"Saves a model state dictionary.\"\"\"\n",
        "    fname = '_'.join(['best', best_stat_name, 'weights.pt'])\n",
        "    filepath =  os.path.join(PROJECT_PATH, 'models', save_model_dir, fname)\n",
        "    torch.save(model_to_save.state_dict(), filepath)"
      ],
      "metadata": {
        "id": "J5KWbs4h5fVV"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_train(train_dataloader, val_dataloader, model, epochs, use_lr_scheduler=True,\n",
        "              model_name='best_model', process_trace_func=process_trace):\n",
        "    \"\"\"Run a training and evaluation loop of a model for a fixed number of epochs.\"\"\"\n",
        "    print(\"Device: \", DEVICE)\n",
        "    \n",
        "    best_epoch_loss = None\n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    # Construct an optimizer\n",
        "    optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9)\n",
        "    # Construct a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3)\n",
        "\n",
        "    if not use_lr_scheduler:\n",
        "       optimizer = torch.optim.Adam(params)\n",
        "       lr_scheduler = None  \n",
        "\n",
        "    with profile(activities=[PROFACTIVITY], profile_memory=True, \n",
        "                 record_shapes=True, schedule=PROFILE_SCHEDULE, on_trace_ready=process_trace_func) as prof:\n",
        "        with record_function('model_inference'):  \n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                print(f\"EPOCH [{epoch+1}/{epochs}]:\")\n",
        "\n",
        "                # Training step\n",
        "                print(\"TRAIN:\")\n",
        "                train_res = train_one_epoch(train_dataloader, model, optimizer, DEVICE)\n",
        "                print(\"  epoch loss: {0}:\\n    {1}\".format(train_res['epoch_loss'], \n",
        "                                                                train_res['epoch_dict_losses']))\n",
        "\n",
        "                if lr_scheduler is not None:\n",
        "                    lr_scheduler.step()        \n",
        "                \n",
        "                # Evaluation step\n",
        "                print(\"EVAL:\")\n",
        "                eval_res = eval_one_epoch(val_dataloader, model, DEVICE)\n",
        "                print(\"  about {0:%} of objects have been detected\\n  epoch score: {1}\".format(\n",
        "                    eval_res['epoch_precent_detected_obj'], \n",
        "                    eval_res['epoch_score'])) \n",
        "                \n",
        "                # Save a model with the minimum epoch loss value\n",
        "                if  best_epoch_loss is None:\n",
        "                    best_epoch_loss = train_res['epoch_loss']\n",
        "                elif best_epoch_loss > train_res['epoch_loss']:\n",
        "                    best_epoch_loss = train_res['epoch_loss']\n",
        "                    save_model_state(model, model_name, 'loss')\n",
        "                    print(f\"Model is saved! --- The best loss: {best_epoch_loss}\")\n",
        "                \n",
        "                print(\"-\" * 60)\n",
        "                prof.step()\n",
        "\n",
        "    print(\"DONE!\")\n",
        "    return {'train_res': train_res,\n",
        "            'eval_res': eval_res}"
      ],
      "metadata": {
        "id": "CH1CWSQ2hHco"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.2. Data Loading"
      ],
      "metadata": {
        "id": "w68OsKixD8Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageBBoxDataset(Dataset):\n",
        "    \"\"\"A Dataset from csv to detect objects in images.\"\"\"\n",
        "    def __init__(self, csv_file_path, img_dir_path, bbox_path, \n",
        "                 img_transforms=None, bbox_transform=None):\n",
        "        self.img_dir_path = img_dir_path\n",
        "        self.img_df = pd.read_csv(csv_file_path)\n",
        "        self.bbox_df = pd.read_csv(bbox_path)\n",
        "        self.img_transforms = img_transforms\n",
        "        self.bbox_transform = bbox_transform # (bbox_transform_fn, *bbox_transform_args) \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.img_df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_df.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir_path, img_name)\n",
        "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        bboxes = self.bbox_df.loc[(self.bbox_df.image_name == img_name), \n",
        "                                 ['bbox_x', 'bbox_y', 'bbox_width', 'bbox_height']].values\n",
        "        labels = torch.ones((bboxes.shape[0],), dtype=torch.int64) \n",
        "\n",
        "        if self.img_transforms:\n",
        "            aug = self.img_transforms(image=image, bboxes=bboxes, labels=labels)\n",
        "            image = aug['image']\n",
        "            bboxes = aug['bboxes']\n",
        "            \n",
        "            # # Save an applied image transformation\n",
        "            # if 'replay' in aug:\n",
        "                # with open(os.path.join(PROJECT_PATH, 'results', 'replay-aug-transforms.json'), 'w') as f:\n",
        "                #     f.write(str(aug['replay']))\n",
        "                 \n",
        "        image = T.ToTensor()(image)\n",
        "        bboxes = torch.as_tensor(bboxes, dtype=torch.float)\n",
        "\n",
        "        if self.bbox_transform:\n",
        "            bboxes = self.bbox_transform[0](bboxes, *self.bbox_transform[1:])            \n",
        "             \n",
        "        target = {'boxes': bboxes,\n",
        "                  'labels': labels}\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "Ut1-3E3yT_JZ"
      },
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_params = {\n",
        "    'img_dir_path': IMAGE_PATH,\n",
        "    'bbox_path': BBOX_DATA_PATH, \n",
        "    'bbox_transform': (box_convert, 'xywh', 'xyxy')\n",
        "}\n",
        "\n",
        "train_dataset = ImageBBoxDataset(TRAIN_FILE_PATH, **dataset_params) # img_transforms=get_image_transforms('image-aug-tansforms')\n",
        "test_dataset = ImageBBoxDataset(TEST_FILE_PATH, **dataset_params)"
      ],
      "metadata": {
        "id": "N5w7fUdNcC_v"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "train_ids, val_ids = stratified_group_train_test_split(train_dataset.img_df['Name'], \n",
        "                                                       train_dataset.img_df['Number_HSparrows'], \n",
        "                                                       train_dataset.img_df['Author'])"
      ],
      "metadata": {
        "id": "67GNFNk9c4PV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67baa9b-0615-4630-f47d-7b317aaa603a"
      },
      "execution_count": 447,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:880: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dl_params = {\n",
        "    'batch_size': 2,\n",
        "    'collate_fn': collate_batch\n",
        "} # num_workers=4\n",
        "\n",
        "train_dl = DataLoader(Subset(train_dataset, train_ids), shuffle=True, **dl_params)\n",
        "val_dl = DataLoader(Subset(train_dataset, val_ids), **dl_params)\n",
        "test_dl = DataLoader(test_dataset, **dl_params)"
      ],
      "metadata": {
        "id": "O2Zefwxqfx3k"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Uncomment to show a sample image with bounding boxes on it\n",
        "# sample_img, sample_target = train_dataset[random.randint(0, len(train_dataset)-1)]\n",
        "# draw_bboxes_on_image(sample_img.mul(255).to(torch.uint8), bboxes=sample_target['boxes'])"
      ],
      "metadata": {
        "id": "sdlO14XcQyz5"
      },
      "execution_count": 449,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.2. Model Loading and Training"
      ],
      "metadata": {
        "id": "JViQy7hMEmrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import (fasterrcnn_resnet50_fpn, fcos_resnet50_fpn,\n",
        "                                          fasterrcnn_mobilenet_v3_large_fpn,\n",
        "                                          fasterrcnn_mobilenet_v3_large_320_fpn)\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.fcos import FCOSClassificationHead"
      ],
      "metadata": {
        "id": "IUmFnfuUhx9K"
      },
      "execution_count": 450,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set general parameters for model training\n",
        "NUM_CLASSES = 2 # 1 class (house sparrow) + background\n",
        "EPOCHS = 1\n",
        "DRAW_IMG_IDX = 255"
      ],
      "metadata": {
        "id": "8mw5197-QAmw"
      },
      "execution_count": 451,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Faster R-CNN (ResNet)"
      ],
      "metadata": {
        "id": "Mrzic_3voB0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Faster R-CNN model pre-trained on COCO\n",
        "faster_rcnn_model = fasterrcnn_resnet50_fpn(weights='COCO_V1') \n",
        "            # rpn_nms_thresh=0.7, \n",
        "            # rpn_fg_iou_thresh=0.7,     --> training\n",
        "            # rpn_bg_iou_thresh=0.3,     --> training\n",
        "            # rpn_positive_fraction=0.5,     --> training\n",
        "            # rpn_score_thresh=0.0,     --> inference\n",
        "            # box_score_thresh=0.05,     --> inference\n",
        "            # box_nms_thresh=0.5,     --> inference\n",
        "            # box_detections_per_img=100,     \n",
        "            # box_fg_iou_thresh=0.5,     --> training\n",
        "            # box_bg_iou_thresh=0.5,     --> training\n",
        "            # box_positive_fraction=0.25     --> training\n",
        "\n",
        "            # rpn_anchor_generator=AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "            #                                      aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# Look at the model's pre-trained head\n",
        "print(faster_rcnn_model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgOkp_L8k1K-",
        "outputId": "9e32abd4-c654-45c4-d143-33b98b687477"
      },
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of input features for the predictor\n",
        "in_features = faster_rcnn_model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "faster_rcnn_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Look at the model's new head\n",
        "print(faster_rcnn_model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "id": "XI0SqDnCmsAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177da181-574c-456a-e49e-1ad9490f2ebd"
      },
      "execution_count": 453,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run a model training cycle\n",
        "# faster_rcnn_res = run_train(train_dl, val_dl, faster_rcnn_model, epochs=EPOCHS, \n",
        "#                             use_lr_scheduler=True, model_name='faster_rcnn')"
      ],
      "metadata": {
        "id": "RQjBpudMbFqu"
      },
      "execution_count": 454,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Draw a image with output boxes and scores\n",
        "# draw_image_with_model_outputs(faster_rcnn_res['eval_res'], draw_score_threshold=0.4, \n",
        "#                               image_index=DRAW_IMG_IDX)   "
      ],
      "metadata": {
        "id": "wc5l4X1z5Y1q"
      },
      "execution_count": 455,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. Faster R-CNN (Mob)"
      ],
      "metadata": {
        "id": "OUEGW-qsoI0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Faster R-CNN model pre-trained on COCO\n",
        "faster_rcnn_mob_model = fasterrcnn_mobilenet_v3_large_fpn(weights='COCO_V1')\n",
        "\n",
        "# Look at the model's pre-trained head\n",
        "print(faster_rcnn_mob_model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W-UBc-NobUb",
        "outputId": "62bc689a-94bd-4816-b5cf-e0851d8b1e18"
      },
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of input features for the predictor\n",
        "in_features_mob = faster_rcnn_mob_model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "faster_rcnn_mob_model.roi_heads.box_predictor = FastRCNNPredictor(in_features_mob, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Look at the model's new head\n",
        "print(faster_rcnn_mob_model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU3wo-a_ohyh",
        "outputId": "e50a24c7-b1ce-4f27-9205-3b5f4e10b5a3"
      },
      "execution_count": 457,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run a model training cycle\n",
        "# faster_rcnn_mob_res = run_train(train_dl, val_dl, faster_rcnn_mob_model, epochs=EPOCHS, \n",
        "#                                 use_lr_scheduler=True, model_name='faster_rcnn_mob')"
      ],
      "metadata": {
        "id": "SJAJRdCGazgT"
      },
      "execution_count": 458,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Draw a image with output boxes and scores\n",
        "# draw_image_with_model_outputs(faster_rcnn_mob_res['eval_res'], draw_score_threshold=0.4, \n",
        "#                               image_index=DRAW_IMG_IDX)   "
      ],
      "metadata": {
        "id": "1hreBpmn8igN"
      },
      "execution_count": 459,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Faster R-CNN (Mob320)"
      ],
      "metadata": {
        "id": "cxuWRv3Ho0WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Faster R-CNN model pre-trained on COCO\n",
        "faster_rcnn_mob320_model = fasterrcnn_mobilenet_v3_large_320_fpn(weights='COCO_V1')\n",
        "\n",
        "# Look at the model's pre-trained head\n",
        "print(faster_rcnn_mob320_model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiLO8q4co5UO",
        "outputId": "8f738d96-6b9e-463a-f5c5-849a0755a228"
      },
      "execution_count": 460,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of input features for the predictor\n",
        "in_features_mob320 = faster_rcnn_mob320_model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "faster_rcnn_mob320_model.roi_heads.box_predictor = FastRCNNPredictor(in_features_mob320, num_classes=NUM_CLASSES)\n",
        "\n",
        "# Look at the model's new head\n",
        "print(faster_rcnn_mob320_model.roi_heads.box_predictor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wtNFsFGo_AD",
        "outputId": "3545a268-9245-4f55-9e3b-f8e32ca34d35"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastRCNNPredictor(\n",
            "  (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run a model training cycle\n",
        "# faster_rcnn_mob320_res = run_train(train_dl, val_dl, faster_rcnn_mob320_model, epochs=EPOCHS, \n",
        "#                                    use_lr_scheduler=True, model_name='faster_rcnn_mob320')"
      ],
      "metadata": {
        "id": "eiwKCbxoakLC"
      },
      "execution_count": 462,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Draw a image with output boxes and scores\n",
        "# draw_image_with_model_outputs(faster_rcnn_mob320_res['eval_res'], draw_score_threshold=0.4, \n",
        "#                               image_index=DRAW_IMG_IDX) "
      ],
      "metadata": {
        "id": "tJGI6YBZ8v5Q"
      },
      "execution_count": 463,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 4. FCOS"
      ],
      "metadata": {
        "id": "W7l6nmRGocEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a FCOS model pre-trained on COCO\n",
        "fcos_model = fcos_resnet50_fpn(weights='COCO_V1') \n",
        "                        # score_thresh=0.2, \n",
        "                        # nms_thresh=0.6, \n",
        "                        # detections_per_img=100,\n",
        "                        # center_sampling_radius=1.5,\n",
        "\n",
        "                        # anchor_generator = AnchorGenerator(sizes=((8,), (16,), (32,), (64,), (128,)),\n",
        "                        #                                    aspect_ratios=((1.0,),))\n",
        "\n",
        "# Look at the model's pre-trained classification head\n",
        "print(fcos_model.head.classification_head) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my_Tj-3cCAsS",
        "outputId": "f386cef4-a309-4d7d-9c3c-d2624f2da073"
      },
      "execution_count": 464,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FCOSClassificationHead(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (8): ReLU()\n",
            "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (11): ReLU()\n",
            "  )\n",
            "  (cls_logits): Conv2d(256, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change classification_head for prediction the given number of classes\n",
        "fcos_model.head.classification_head.num_classes = NUM_CLASSES\n",
        "\n",
        "# Get number of input channels for the classifier\n",
        "in_channels = fcos_model.head.classification_head.cls_logits.in_channels\n",
        "\n",
        "# Replace the pre-trained classification top layer with a new one\n",
        "num_anchors = fcos_model.head.classification_head.num_anchors\n",
        "fcos_model.head.classification_head.cls_logits = torch.nn.Conv2d(in_channels, NUM_CLASSES * num_anchors, \n",
        "                                                                 kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "# Look at the model's classification head\n",
        "print(fcos_model.head.classification_head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN8jSjVwq5k0",
        "outputId": "ae33e1a4-737b-423c-9932-e14774067bf8"
      },
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FCOSClassificationHead(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (8): ReLU()\n",
            "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "    (11): ReLU()\n",
            "  )\n",
            "  (cls_logits): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a model training cycle\n",
        "# fcos_res = run_train(train_dl, val_dl, fcos_model, epochs=EPOCHS, \n",
        "#                      use_lr_scheduler=False, model_name='fcos')"
      ],
      "metadata": {
        "id": "FiKFrIzUZ9Sa"
      },
      "execution_count": 466,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Draw a image with output boxes and scores\n",
        "# draw_image_with_model_outputs(fcos_res['eval_res'], draw_score_threshold=0.4, \n",
        "#                               image_index=DRAW_IMG_IDX) "
      ],
      "metadata": {
        "id": "DHklH5Xe83_H"
      },
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.2.2. Check train functions"
      ],
      "metadata": {
        "id": "nLN1XRK8jlGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_dl = DataLoader(Subset(train_dataset, [0, 1, 255]), **dl_params)"
      ],
      "metadata": {
        "id": "TDrFEbYNfrGQ"
      },
      "execution_count": 468,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models: faster_rcnn_model, faster_rcnn_mob_model, faster_rcnn_mob320_model, fcos_model\n",
        "check_res = run_train(check_dl, check_dl, faster_rcnn_mob_model, epochs=EPOCHS, use_lr_scheduler=True)"
      ],
      "metadata": {
        "id": "IscXdGOzy5bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6410d08d-7e2c-4877-eabd-6cedbd56749a"
      },
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n",
            "EPOCH [1/1]:\n",
            "TRAIN:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 2/2 [00:08<00:00,  4.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch loss: 1.9349839091300964:\n",
            "    {'loss_classifier': 0.712329626083374, 'loss_box_reg': 1.0923421382904053, 'loss_objectness': 0.09301595701253973, 'loss_rpn_box_reg': 0.03729613497853279}\n",
            "EVAL:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u001b[34m██████████\u001b[0m| 2/2 [00:03<00:00,  1.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  about 1650.000000% of objects have been detected\n",
            "  epoch score: 0.5561054535210133\n",
            "------------------------------------------------------------\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                              aten::mul         8.62%     969.079ms         8.62%     969.724ms       1.142ms       2.86 Gb       2.86 Gb           849  \n",
            "                                              aten::add         7.05%     792.431ms         7.10%     798.820ms       1.437ms       2.21 Gb       2.21 Gb           556  \n",
            "                                           aten::conv2d         0.02%       2.397ms        23.77%        2.673s       8.909ms       2.10 Gb           0 b           300  \n",
            "                                      aten::convolution         0.04%       4.393ms        23.74%        2.670s       8.901ms       2.10 Gb           0 b           300  \n",
            "                                     aten::_convolution         0.05%       5.214ms        23.71%        2.666s       8.887ms       2.10 Gb           0 b           300  \n",
            "                                            aten::empty         0.06%       7.242ms         0.06%       7.242ms       4.831us       1.61 Gb       1.61 Gb          1499  \n",
            "                                          aten::resize_         0.03%       3.451ms         0.03%       3.451ms       7.825us       1.53 Gb       1.53 Gb           441  \n",
            "                                      aten::thnn_conv2d         0.01%       1.216ms        11.24%        1.264s       5.851ms       1.33 Gb           0 b           216  \n",
            "                             aten::_slow_conv2d_forward        11.14%        1.253s        11.23%        1.263s       5.845ms       1.33 Gb           0 b           216  \n",
            "                               aten::mkldnn_convolution        12.39%        1.394s        12.42%        1.397s      16.628ms     784.55 Mb           0 b            84  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 11.246s\n",
            "\n",
            "-----------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "-----------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "      aten::empty         8.03%      48.000us         8.03%      48.000us      24.000us         268 b         268 b             2  \n",
            "      aten::zeros         4.01%      24.000us         8.86%      53.000us      53.000us           4 b           0 b             1  \n",
            "      aten::zero_         0.33%       2.000us         0.33%       2.000us       2.000us           0 b           0 b             1  \n",
            "    ProfilerStep*        87.63%     524.000us        91.14%     545.000us     545.000us          -4 b        -268 b             1  \n",
            "-----------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 598.000us\n",
            "\n",
            "DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9vnpvBy0GA1"
      },
      "execution_count": 469,
      "outputs": []
    }
  ]
}